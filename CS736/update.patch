diff --git a/schedulers/cfs/cfs_scheduler.cc b/schedulers/cfs/cfs_scheduler.cc
index 380fa5f..7239701 100644
--- a/schedulers/cfs/cfs_scheduler.cc
+++ b/schedulers/cfs/cfs_scheduler.cc
@@ -20,7 +20,6 @@
 #include <string>
 #include <utility>
 #include <vector>
-#include <map>
 
 #include "absl/functional/any_invocable.h"
 #include "absl/numeric/int128.h"
@@ -45,11 +44,8 @@
 ABSL_FLAG(bool, experimental_enable_idle_load_balancing, true,
           "Experimental flag to enable idle load balancing.");
 
-
-
 namespace ghost {
 
-
 void PrintDebugTaskMessage(std::string message_name, CpuState* cs,
                            CfsTask* task) {
   DPRINT_CFS(2, absl::StrFormat(
@@ -488,53 +484,94 @@ void CfsScheduler::TaskDead(CfsTask* task, const Message& msg) {
 void CfsScheduler::TaskYield(CfsTask* task, const Message& msg) {
   const ghost_msg_payload_task_yield* payload =
       static_cast<const ghost_msg_payload_task_yield*>(msg.payload());
-  Cpu cpu = topology()->cpu(payload->cpu);
+  Cpu cpu = topology()->cpu(MyCpu());
   CpuState* cs = cpu_state(cpu);
   PrintDebugTaskMessage("TaskYield", cs, task);
   cs->run_queue.mu_.AssertHeld();
 
-  // Kick the task off-cpu.
-  CHECK_EQ(cs->current, task);
-  PutPrevTask();
+  // If this task is not from a switchto chain, it should be the current task on
+  // this CPU.
+  if (!payload->from_switchto) {
+    CHECK_EQ(cs->current, task);
+  }
 
-  if (payload->from_switchto) {
-    Cpu cpu = topology()->cpu(payload->cpu);
-    PingCpu(cpu);
+  // The task should be in kDequeued state because only a currently running
+  // task can yield.
+  CHECK(task->task_state.OnRqDequeued());
+
+  // Updates the task state accordingly. This is safe because this task should
+  // be associated with this CPU's agent and protected by this CPU's RQ lock.
+  PutPrevTask(task);
+
+  // This task was the last task in a switchto chain on a remote CPU. We should
+  // ping the remote CPU to schedule a new task.
+  if (payload->cpu != cpu.id()) {
+    CHECK(payload->from_switchto);
+    PingCpu(topology()->cpu(payload->cpu));
   }
 }
 
 void CfsScheduler::TaskBlocked(CfsTask* task, const Message& msg) {
   const ghost_msg_payload_task_blocked* payload =
       static_cast<const ghost_msg_payload_task_blocked*>(msg.payload());
-  Cpu cpu = topology()->cpu(payload->cpu);
+  Cpu cpu = topology()->cpu(MyCpu());
   CpuState* cs = cpu_state(cpu);
   PrintDebugTaskMessage("TaskBlocked", cs, task);
   cs->run_queue.mu_.AssertHeld();
 
-  CHECK_EQ(cs->current, task);
-  task->task_state.SetState(CfsTaskState::State::kBlocked);
+  // If this task is not from a switchto chain, it should be the current task on
+  // this CPU.
+  if (!payload->from_switchto) {
+    CHECK_EQ(cs->current, task);
+  }
 
-  if (payload->from_switchto) {
-    Cpu cpu = topology()->cpu(payload->cpu);
-    PingCpu(cpu);
+  // Updates the task state accordingly. This is safe because this task should
+  // be associated with this CPU's agent and protected by this CPU's RQ lock.
+  if (cs->current == task) {
+    cs->current = nullptr;
+  }
+
+  task->task_state.SetState(CfsTaskState::State::kBlocked);
+  // No need to update OnRq state to kDequeued because the task should already
+  // be in kDequeued state because only a currently running task can block and
+  // it should be in kDequeued state.
+  CHECK(task->task_state.OnRqDequeued());
+
+  // This task was the last task in a switchto chain on a remote CPU. We should
+  // ping the remote CPU to schedule a new task.
+  if (payload->cpu != cpu.id()) {
+    CHECK(payload->from_switchto);
+    PingCpu(topology()->cpu(payload->cpu));
   }
 }
 
 void CfsScheduler::TaskPreempted(CfsTask* task, const Message& msg) {
   const ghost_msg_payload_task_preempt* payload =
       static_cast<const ghost_msg_payload_task_preempt*>(msg.payload());
-  Cpu cpu = topology()->cpu(payload->cpu);
+  Cpu cpu = topology()->cpu(MyCpu());
   CpuState* cs = cpu_state(cpu);
   PrintDebugTaskMessage("TaskPreempted", cs, task);
   cs->run_queue.mu_.AssertHeld();
 
-  // Kick the task off-cpu.
-  CHECK_EQ(cs->current, task);
-  PutPrevTask();
+  // If this task is not from a switchto chain, it should be the current task on
+  // this CPU.
+  if (!payload->from_switchto) {
+    CHECK_EQ(cs->current, task);
+  }
 
-  if (payload->from_switchto) {
-    Cpu cpu = topology()->cpu(payload->cpu);
-    PingCpu(cpu);
+  // The task should be in kDequeued state because only a currently running
+  // task can be preempted.
+  CHECK(task->task_state.OnRqDequeued());
+
+  // Updates the task state accordingly. This is safe because this task should
+  // be associated with this CPU's agent and protected by this CPU's RQ lock.
+  PutPrevTask(task);
+
+  // This task was the last task in a switchto chain on a remote CPU. We should
+  // ping the remote CPU to schedule a new task.
+  if (payload->cpu != cpu.id()) {
+    CHECK(payload->from_switchto);
+    PingCpu(topology()->cpu(payload->cpu));
   }
 }
 
@@ -543,8 +580,12 @@ void CfsScheduler::TaskSwitchto(CfsTask* task, const Message& msg) {
   PrintDebugTaskMessage("TaskSwitchTo", cs, task);
   cs->run_queue.mu_.AssertHeld();
 
+  CHECK_EQ(cs->current, task);
   task->task_state.SetState(CfsTaskState::State::kBlocked);
-  task->task_state.SetOnRq(CfsTaskState::OnRq::kDequeued);
+  // No need to update OnRq state to kDequeued because the task should be on
+  // CPU and therefore in kDequeued state.
+  CHECK(task->task_state.OnRqDequeued());
+  cs->current = nullptr;
 }
 
 // Disable thread safety analysis as this function is called with rq lock held
@@ -567,12 +608,16 @@ void CfsScheduler::CheckPreemptTick(const Cpu& cpu)
   }
 }
 
-void CfsScheduler::PutPrevTask() {
+void CfsScheduler::PutPrevTask(CfsTask* task) {
   CpuState* cs = &cpu_states_[MyCpu()];
   cs->run_queue.mu_.AssertHeld();
 
-  CfsTask* task = cs->current;
-  cs->current = nullptr;
+  CHECK_NE(task, nullptr);
+
+  // If this task is currently running, kick it off-cpu.
+  if (cs->current == task) {
+    cs->current = nullptr;
+  }
 
   // We have a notable deviation from the upstream's behavior here. In upstream,
   // put_prev_task does not update the state, while we update the state here.
@@ -724,36 +769,7 @@ void CfsScheduler::CfsSchedule(const Cpu& cpu, BarrierToken agent_barrier,
   CpuState* cs = cpu_state(cpu);
 
   CfsTask* prev = cs->current;
-  //const pid_t previd = (prev->gtid).tid();
-  //printf(prev->gtid.describe());
-  //PrintDebugTaskMessage("test", cs, prev);
-  //Gtid prevGtid;
-  //prevprev->gtid;
-  //pid_t prevTid = prevGtid.tid();
-  //printf("\nThis is task tid %d", prevTid);
-  int SEAL_break = 0;
-  if (prev){
-    int val;
-    //printf("tid is %d\n", (prev->gtid).tid());
-    if (prev->seal_prio == 0){
-      val = readValue((prev->gtid).tid());
-      prev->seal_prio = val;
-      //printf("read value of tid %d is %d\n", (prev->gtid).tid(), val);
-    }else{
-      val = prev->seal_prio;
-    }
-    //int val = 0;
-    //printf("read value of tid %d is %d\n", (prev->gtid).tid(), val);
-    //read time slice T
-    uint64_t T = timeslice[val];
-    if(val > 0){
-      if(prev->runtime_at_current_time_slice < T){
-        SEAL_break = 1;
-      }else{
-        prev->runtime_at_current_time_slice = 0;
-      }
-    }
-  }
+
   if (prio_boost) {
     // If we are currently running a task, we need to put it back onto the
     // queue.
@@ -778,7 +794,6 @@ void CfsScheduler::CfsSchedule(const Cpu& cpu, BarrierToken agent_barrier,
         case CfsTaskState::State::kRunning:
           cs->run_queue.PutPrevTask(prev);
           prev->task_state.SetState(CfsTaskState::State::kRunnable);
-          //prev->task_state.SetState(CfsTaskState::State::kRunnable);
           break;
       }
 
@@ -802,11 +817,7 @@ void CfsScheduler::CfsSchedule(const Cpu& cpu, BarrierToken agent_barrier,
   }
 
   cs->run_queue.mu_.Lock();
-
-  //CfsTask* next = cs->run_queue.PickNextTask(prev, allocator(), cs);
-  CfsTask* next = cs->run_queue.PickNextTaskSEALs(prev, allocator(), cs, SEAL_break);
-
-  //CfsTask* next = cs->run_queue.PickNextTask(prev, allocator(), cs);
+  CfsTask* next = cs->run_queue.PickNextTask(prev, allocator(), cs);
   cs->run_queue.mu_.Unlock();
 
   if (!next && idle_load_balancing_) {
@@ -816,7 +827,7 @@ void CfsScheduler::CfsSchedule(const Cpu& cpu, BarrierToken agent_barrier,
   cs->current = next;
 
   if (next) {
-    DPRINT_CFS(2, absl::StrFormat("[%s]: Picked via PickNextTask",
+    DPRINT_CFS(3, absl::StrFormat("[%s]: Picked via PickNextTask",
                                   next->gtid.describe()));
 
     req->Open({
@@ -858,10 +869,8 @@ void CfsScheduler::CfsSchedule(const Cpu& cpu, BarrierToken agent_barrier,
       //         = wall_runtime * 2^10 / (2^32 / precomputed_inverse_weight)
       //         = wall_runtime * precomputed_inverse_weight / 2^22
       uint64_t runtime = next->status_word.runtime() - before_runtime;
-      //printf("runtime for this time slice %ld\n", runtime);
       next->vruntime += absl::Nanoseconds(static_cast<uint64_t>(
           static_cast<absl::uint128>(next->inverse_weight) * runtime >> 22));
-      next-> runtime_at_current_time_slice += runtime;
     } else {
       GHOST_DPRINT(3, stderr, "CfsSchedule: commit failed (state=%d)",
                    req->state());
@@ -1061,67 +1070,6 @@ void CfsRq::PutPrevTask(CfsTask* task) ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
   InsertTaskIntoRq(task);
 }
 
-CfsTask* CfsRq::PickNextTaskSEALs(CfsTask* prev, TaskAllocator<CfsTask>* allocator,
-                             CpuState* cs, int seals_break) {
-  // Check if we can just keep running the current task.
-  if (prev && prev->task_state.IsRunning() && seals_break) {
-    return prev;
-  }
-  if (prev && prev->task_state.IsRunning() && !cs->preempt_curr) {
-    return prev;
-  }
-
-  // Past here, we will return a new task to run, so reset our preemption flag.
-  cs->preempt_curr = false;
-
-  // Check what happened to our previously running task and reconcile our
-  // runqueue. No scheduling decision is made here unless our prev task still
-  // wants to be oncpu, then we check if it needs to be preempted or not. If
-  // it does not, we just transact prev if it does, then we go through to
-  // PickNextTask.
-  if (prev) {
-    switch (prev->task_state.GetState()) {
-      case CfsTaskState::State::kNumStates:
-        CHECK(false);
-        break;
-      case CfsTaskState::State::kBlocked:
-        break;
-      case CfsTaskState::State::kDone:
-        DequeueTask(prev);
-        allocator->FreeTask(prev);
-        break;
-      case CfsTaskState::State::kRunnable:
-        PutPrevTask(prev);
-        break;
-      case CfsTaskState::State::kRunning:
-        // We had the preempt curr flag set, so we need to put our current task
-        // back into the rq.
-        PutPrevTask(prev);
-        prev->task_state.SetState(CfsTaskState::State::kRunnable);
-        break;
-    }
-  }
-
-  // First, we reconcile our CpuState with the messaging relating to prev.
-  if (IsEmpty()) {
-    UpdateMinVruntime(cs);
-    return nullptr;
-  }
-
-  CfsTask* task = LeftmostRqTask();
-  DequeueTask(task);
-  task->task_state.SetState(CfsTaskState::State::kRunning);
-  task->runtime_at_first_pick_ns = task->status_word.runtime();
-
-  // min_vruntime is used for Enqueing new tasks. We want to place them at
-  // at least the current moment in time. Placing them before min_vruntime,
-  // would give them an inordinate amount of runtime on the CPU as they would
-  // need to catch up to other tasks that have accummulated a large runtime.
-  // For easy access, we cache the value.
-  UpdateMinVruntime(cs);
-  return task;
-}
-
 CfsTask* CfsRq::PickNextTask(CfsTask* prev, TaskAllocator<CfsTask>* allocator,
                              CpuState* cs) {
   // Check if we can just keep running the current task.
diff --git a/schedulers/cfs/cfs_scheduler.h b/schedulers/cfs/cfs_scheduler.h
index 834876f..6cb146c 100644
--- a/schedulers/cfs/cfs_scheduler.h
+++ b/schedulers/cfs/cfs_scheduler.h
@@ -14,12 +14,6 @@
 #include <ostream>
 #include <set>
 
-#include <fstream>
-#include <fcntl.h>
-#include <unistd.h>
-#include <sys/file.h>
-
-
 #include "absl/container/flat_hash_map.h"
 #include "absl/functional/any_invocable.h"
 #include "absl/strings/str_format.h"
@@ -34,76 +28,6 @@ static const absl::Time start = absl::Now();
 
 namespace ghost {
 
-inline std::map<int, int64_t> timeslice = {
-    {0, 8000},
-    {1, 200000},
-    {2, 100000},
-    {3, 10000},
-    {4, 8000},
-    {5, 8000}
-};
-/*
-inline int readValue(int fileName) {
-    std::string filename = "/tidinfo/" + std::to_string(fileName) + ".txt"; // 构造文件路径
-    std::ifstream inputFile(filename);
-    int number = 0;
-    try {
-        if (inputFile.is_open()) {
-            std::string data;
-            inputFile >> data;
-            number = std::stoi(data); // 尝试将字符串转换为整数
-            inputFile.close();
-        } else {
-            std::cerr << "open file error\n";
-        }
-    } catch (const std::invalid_argument& e) {
-        std::cerr << "\nInvalid data in the file: " << e.what();
-        number = 0;
-    }
-}
-*/
-
-inline int readValue(int fileName) {
-    std::string filename = "/tidinfo/" + std::to_string(fileName) + ".txt"; // 构造文件路径
-
-    int fileDescriptor = open(filename.c_str(), O_RDONLY);
-    if (fileDescriptor == -1) {
-        std::cerr << "Error opening file." << std::endl;
-        return 0;
-    }
-
-    // 请求共享锁（读取锁）
-    if (flock(fileDescriptor, LOCK_SH) == -1) {
-        std::cerr << "Error locking file for reading." << std::endl;
-        close(fileDescriptor);
-        return 0;
-    }
-
-    int number = 0;
-    try {
-        std::ifstream inputFile(filename);
-        if (inputFile.is_open()) {
-            std::string data;
-            inputFile >> data;
-            number = std::stoi(data); // 尝试将字符串转换为整数
-            inputFile.close();
-        } else {
-            std::cerr << "Open file error\n";
-        }
-    } catch (const std::invalid_argument& e) {
-        std::cerr << "\nInvalid data in the file: " << e.what();
-        number = 0;
-    }
-
-    // 释放锁
-    if (flock(fileDescriptor, LOCK_UN) == -1) {
-        std::cerr << "Error unlocking file after reading." << std::endl;
-    }
-
-    close(fileDescriptor);
-
-    return number;
-}
 // We could use just "enum class", but embedding an enum (which is implicitly
 // convertable to an int) makes a lot of the debugging code simpler. We could do
 // some hackery like static_cast<typename
@@ -156,6 +80,7 @@ class CfsTaskState {
   // Convenience functions for accessing on_rq state.
   inline bool OnRqQueued() const { return on_rq_ == OnRq::kQueued; }
   inline bool OnRqMigrating() const { return on_rq_ == OnRq::kMigrating; }
+  inline bool OnRqDequeued() const { return on_rq_ == OnRq::kDequeued; }
 
   // Accessors.
   State GetState() const { return state_; }
@@ -294,32 +219,10 @@ struct CfsTask : public Task<> {
   // function as a template parameter. Technically, this doesn't have to be
   // inside of the struct, but it seems logical to keep this here.
   static bool Less(CfsTask* a, CfsTask* b) {
-    //read priority of task a and task b;
-    int pa = 0;
-    int pb = 0;
-    pa = a->seal_prio;
-    pb = b->seal_prio;
-    //printf("taska %d, taskb %d\n", pa, pb);
-    if (pa == 0){
-      pa = readValue((a->gtid).tid());
-    }
-    if (pb == 0){
-      pb = readValue((b->gtid).tid());
-    }
-    //if(a){
-    //  pa = readValue((a->gtid).tid());
-    //}
-    //if(b){
-    //  pb = readValue((b->gtid).tid());
-    //}
-    if (pa != pb){
-      return pa < pb;
-    }else{
-      if (a->vruntime == b->vruntime) {
-        return (uintptr_t)a < (uintptr_t)b;
-      }
-      return a->vruntime < b->vruntime;
+    if (a->vruntime == b->vruntime) {
+      return (uintptr_t)a < (uintptr_t)b;
     }
+    return a->vruntime < b->vruntime;
   }
 
   CfsTaskState task_state =
@@ -341,9 +244,6 @@ struct CfsTask : public Task<> {
   // has been running.
   absl::Duration vruntime;
 
-  //
-  uint64_t runtime_at_current_time_slice = 0;
-  int seal_prio = 0;
   // runtime_at_first_pick is how much runtime this task had at its initial
   // picking. This timestamp does not change unless we are put back in the
   // runqueue. IOW, if we bounce between oncpu and put_prev_task_elision_,
@@ -385,9 +285,6 @@ class CfsRq {
   // prev. PickNextTask sets the state of its returned task to kOnCpu.
   CfsTask* PickNextTask(CfsTask* prev, TaskAllocator<CfsTask>* allocator,
                         CpuState* cs) ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);
-  
-  CfsTask* PickNextTaskSEALs(CfsTask* prev, TaskAllocator<CfsTask>* allocator,
-                        CpuState* cs, int seals_break) ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);
 
   // Enqueues a new task or a task that is coming from being blocked.
   void EnqueueTask(CfsTask* task) ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);
@@ -528,8 +425,8 @@ struct CpuState {
   // ID of the cpu.
   int id = -1;
 
-  bool IsIdle() { return current == nullptr; }
-  bool LocklessRqEmpty() { return run_queue.LocklessSize() == 0; }
+  bool IsIdle() const { return current == nullptr; }
+  bool LocklessRqEmpty() const { return run_queue.LocklessSize() == 0; }
 } ABSL_CACHELINE_ALIGNED;
 
 class CfsScheduler : public BasicDispatchScheduler<CfsTask> {
@@ -612,7 +509,7 @@ class CfsScheduler : public BasicDispatchScheduler<CfsTask> {
   void DumpState(const Cpu& cpu, int flags) final;
   std::atomic<bool> debug_runqueue_ = false;
 
-  int CountAllTasks() {
+  int CountAllTasks() const {
     int num_tasks = 0;
     allocator()->ForEachTask([&num_tasks](Gtid gtid, const CfsTask* task) {
       ++num_tasks;
@@ -647,10 +544,11 @@ class CfsScheduler : public BasicDispatchScheduler<CfsTask> {
   // Note: Should be called with this CPU's rq mutex lock held.
   void CheckPreemptTick(const Cpu& cpu);
 
-  // Kicks the current task off-cpu and puts into this CPU's run queue or
-  // initiate its migration if this CPU is no longer eligible as per its
-  // affinity mask.
-  void PutPrevTask();
+  // Kicks a given task off-cpu and puts into this CPU's run queue or initiate
+  // its migration if this CPU is no longer eligible as per its affinity mask.
+  // If the task was on-cpu (cs->current == task), cs->current will be reset as
+  // a side effect.
+  void PutPrevTask(CfsTask* task);
 
   // CfsSchedule looks at the current cpu state and its run_queue, decides what
   // to run next, and then commits a txn. REQUIRES: Called after all messages
